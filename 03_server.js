// =============================================================================
// Microservicio REST: Chatbot Documental (RAG + LLM local)
// Idioma: Espa√±ol
// =============================================================================

import express from 'express';
import dotenv from 'dotenv';
import mongoose from 'mongoose';
import path from 'path';
import { fileURLToPath } from 'url';
import { pipeline } from '@xenova/transformers';

// Carga de variables de entorno
dotenv.config();

// __dirname en ES Modules
const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

// App Express
const app = express();
app.use(express.json());

// Variables de entorno requeridas
const PORT = Number(process.env.PORT || 3000);
const MONGO_URI = process.env.MONGO_URI || '';
const MONGO_DB = process.env.MONGO_DB || process.env.MONGO_DB_NAME || '';
const MODEL_PATH = process.env.MODEL_PATH ? path.resolve(__dirname, process.env.MODEL_PATH) : '';

// Estado global
let embeddingPipeline = null;  // Modelo de embeddings (Xenova)
let llamaModel = null;         // Instancia del LLM
let LlamaContextClass = null;  // Clases del runtime Llama
let LlamaChatSessionClass = null;
let Knowledge = null;          // Modelo Mongoose

// Esquema de conocimiento
const knowledgeSchema = new mongoose.Schema({
  source: { type: String, required: true },
  content: { type: String, required: true },
  embedding: { type: [Number], required: true },
}, { versionKey: false, strict: true });

// Helper: forzar no-SRV y a√±adir par√°metros de conexi√≥n seguros
function buildMongoUriNoSrv(input) {
  try {
    const u = new URL(input);
    // Si es SRV, intentamos cambiar a mongodb://
    if (u.protocol === 'mongodb+srv:') {
      u.protocol = 'mongodb:'; // fuerza modo no-SRV
    }
    // Eliminar par√°metros potencialmente no soportados
    if (u.searchParams.has('srv')) u.searchParams.delete('srv');
    // Mantener par√°metros opcionales si existen, pero no forzar 'srv'
    if (!u.searchParams.has('appName')) u.searchParams.set('appName', 'inacapito-rag');
    return u.toString();
  } catch {
    // Si falla el parser, devolver tal cual (sin a√±adir 'srv=false')
    return input;
  }
}

// Utilidad: verificar estado de conexi√≥n de Mongoose
function isDbConnected() {
  try {
    return mongoose.connection && mongoose.connection.readyState === 1; // 1 = connected
  } catch {
    return false;
  }
}

// ---------------------------- Inicializaciones ------------------------------
async function connectToMongoDB() {
  console.log('  - Conectando a MongoDB...');

  try {
    const cleanUri = buildMongoUriNoSrv(MONGO_URI);
    await mongoose.connect(cleanUri);
    
    Knowledge = mongoose.models.Knowledge || mongoose.model('Knowledge', knowledgeSchema, 'knowledge');
    console.log('  ‚úÖ [SUCCESS] Conectado a MongoDB');
  } catch (e) {
    console.warn('[WARN] No se pudo conectar a MongoDB. Arrancando sin base de datos.');
    console.warn('Motivo:', e?.message || e);
  }
}

async function initializeEmbeddingModel() {
  console.log('  - Cargando modelo de embeddings (Xenova/all-MiniLM-L6-v2)...');
  embeddingPipeline = await pipeline('feature-extraction', 'Xenova/all-MiniLM-L6-v2');
  console.log('  ‚úÖ Embeddings listos');
}

async function initializeLlamaModel() {
  if (!MODEL_PATH) {
    throw new Error('Falta MODEL_PATH en .env');
  }
  console.log(`  - Cargando LLM desde: ${MODEL_PATH}`);
  
  // Intento 1: node-llama-cpp (m√°s estable)
  try {
    const { LlamaModel, LlamaContext, LlamaChatSession } = await import('node-llama-cpp');
    LlamaContextClass = LlamaContext;
    LlamaChatSessionClass = LlamaChatSession;
    llamaModel = new LlamaModel({ modelPath: MODEL_PATH });
    console.log('  ‚úÖ LLM cargado (node-llama-cpp)');
    return;
  } catch (e1) {
    console.log(`    Intento con node-llama-cpp fall√≥: ${e1.message}`);
  }

  // Intento 2: llama-cpp-node (fallback)
  try {
    const llamaCppMod = await import('llama-cpp-node');
    const LlamaModelClass = llamaCppMod.LlamaModel || llamaCppMod.default;
    LlamaContextClass = llamaCppMod.LlamaContext || llamaCppMod.default?.LlamaContext;
    LlamaChatSessionClass = llamaCppMod.LlamaChatSession || llamaCppMod.default?.LlamaChatSession;
    
    if (!LlamaModelClass) {
      throw new Error('No se encontr√≥ LlamaModel en llama-cpp-node');
    }
    
    llamaModel = new LlamaModelClass({ modelPath: MODEL_PATH });
    console.log('  ‚úÖ LLM cargado (llama-cpp-node)');
    return;
  } catch (e2) {
    console.log(`    Intento con llama-cpp-node fall√≥: ${e2.message}`);
  }

  throw new Error('No se pudo cargar ninguna implementaci√≥n de Llama (node-llama-cpp / llama-cpp-node).');
}

// ----------------------------- Utilidades RAG -------------------------------
async function generateEmbedding(texto) {
  if (!embeddingPipeline) {
    // Sin embeddings, no podemos hacer b√∫squeda vectorial; el endpoint gestionar√° la negaci√≥n
    return [];
  }
  const out = await embeddingPipeline(texto, { pooling: 'mean', normalize: true });
  return Array.from(out.data);
}

async function vectorSearch(queryEmbedding, topK = 4) {
  // Si no hay DB o modelo Knowledge, devolvemos array vac√≠o como fallback
  if (!Knowledge || !isDbConnected()) return [];
  
  const results = await Knowledge.aggregate([
    { $vectorSearch: { index: 'vector_index', path: 'embedding', queryVector: queryEmbedding, numCandidates: 100, limit: topK } },
    { $project: { _id: 0, source: 1, content: 1, score: { $meta: 'vectorSearchScore' } } }
  ]);
  return results;
}

async function generateResponse(userQuery, contextChunks) {
  // Fallback si el LLM no est√° disponible
  if (!llamaModel || !LlamaContextClass || !LlamaChatSessionClass) {
    return 'No puedo confirmar eso con los documentos proporcionados.';
  }

  const contexto = contextChunks.map(c => c.content).join('\n\n---\n\n');
  const systemPrompt = 'Eres "Inacapito", un asistente de IA que responde SOLO con el contexto proporcionado. Si no hay informaci√≥n suficiente en el contexto, responde exactamente: "No puedo confirmar eso con los documentos proporcionados." Responde en espa√±ol, claro y conciso.';
  const prompt = `Contexto de documentos:\n${contexto}\n---\nPregunta: "${userQuery}"\n\nRespuesta:`;

  const ctx = new LlamaContextClass({ model: llamaModel, contextSize: 1024 });
  const session = new LlamaChatSessionClass({ context: ctx, systemPrompt });
  const respuesta = await session.prompt(prompt, { temperature: 0.2, maxTokens: 512 });
  return respuesta;
}

// --------------------------------- Rutas ------------------------------------
app.get('/health', (req, res) => {
  res.json({ status: 'ok', time: new Date().toISOString() });
});

app.post('/chat', async (req, res) => {
  try {
    const { query, topK } = req.body || {};
    if (!query || typeof query !== 'string' || query.trim().length === 0) {
      return res.status(400).json({ error: 'Body inv√°lido: se requiere "query" (string)' });
    }

    // Si no hay embeddings, negar directamente
    if (!embeddingPipeline) {
      return res.json({ answer: 'No puedo confirmar eso con los documentos proporcionados.', sources: [], confidence: 'baja' });
    }

    // 1) Embedding del query
    const queryEmbedding = await generateEmbedding(query);

    // 2) Recuperaci√≥n (Vector Search)
    const k = Number.isInteger(topK) && topK > 0 && topK <= 10 ? topK : 4;
    const retrieved = await vectorSearch(queryEmbedding, k);

    // Negaci√≥n estricta si no hay contexto
    if (!retrieved || retrieved.length === 0) {
      return res.json({ answer: 'No puedo confirmar eso con los documentos proporcionados.', sources: [], confidence: 'baja' });
    }

    // 4) Generaci√≥n local con LLM
    const answer = await generateResponse(query, retrieved);

    // 5) Fuentes y confianza
    const sources = [...new Set(retrieved.map(r => r.source).filter(Boolean))];
    const avgScore = retrieved.reduce((acc, r) => acc + (r.score ?? 0), 0) / retrieved.length;
    const confidence = avgScore > 0.9 ? 'alta' : avgScore > 0.8 ? 'media' : 'baja';

    // 6) Negaci√≥n si el modelo la expresa expl√≠citamente
    if (!answer || /no puedo confirmar eso/i.test(answer)) {
      return res.json({ answer: 'No puedo confirmar eso con los documentos proporcionados.', sources: [], confidence: 'baja' });
    }

    return res.json({ answer, sources, confidence });
  } catch (err) {
    console.error('[ERROR /chat]', err);
    return res.status(500).json({ error: 'Error interno procesando la consulta.' });
  }
});

// 404
app.use((req, res) => {
  res.status(404).json({ error: `Ruta no encontrada: ${req.method} ${req.originalUrl}` });
});

// ------------------------------- Arranque -----------------------------------
async function startServer() {
  console.log('üöÄ Iniciando microservicio de chatbot documental...');

  // Intentar conectar a MongoDB, pero no abortar si falla
  try {
    await connectToMongoDB();
  } catch (e) {
    console.warn('[WARN] No se pudo conectar a MongoDB. El chatbot arrancar√° sin base de datos.');
    console.warn('Motivo:', e?.message || e);
  }

  // Inicializar embeddings (si falla, arrancamos igual con negaci√≥n estricta)
  try {
    await initializeEmbeddingModel();
  } catch (e) {
    console.warn('[WARN] No se pudo cargar el modelo de embeddings. Las respuestas negar√°n por falta de contexto.');
    console.warn('Motivo:', e?.message || e);
  }

  // Inicializar LLM (COMENTADO PARA ARRANQUE)
  // try {
  //   await initializeLlamaModel();
  // } catch (e) {
  //   console.warn('[WARN] No se pudo cargar el LLM. Se usar√° modo fallback sin generaci√≥n neural.');
  //   console.warn('Motivo:', e?.message || e);
  // }

  app.listen(PORT, () => {
    console.log(`[SUCCESS] Microservicio escuchando en http://localhost:${PORT}`);
    console.log('Endpoints: POST /chat, GET /health');
    if (!isDbConnected()) console.log('‚ö†Ô∏è  Ejecutando sin conexi√≥n a MongoDB (vectorSearch devolver√° vac√≠o).');
    if (!embeddingPipeline) console.log('‚ö†Ô∏è  Embeddings no disponibles (se devolver√° negaci√≥n).');
    if (!llamaModel) console.log('‚ö†Ô∏è  LLM no disponible (respuesta por fallback).');
  });
}

// Invocaci√≥n con try/catch superior
(async () => {
  try {
    await startServer();
  } catch (e) {
    console.error('\nCRITICAL STARTUP FAILURE');
    console.error(e?.message || e);
    process.exit(1);
  }
})();

// ---------------------------- Cierre Graceful -------------------------------
function shutdown(signal) {
  console.log(`\n${signal} recibido. Cerrando...`);
  mongoose.connection.close(false).then(() => process.exit(0));
}
process.on('SIGINT', () => shutdown('SIGINT'));
process.on('SIGTERM', () => shutdown('SIGTERM'));